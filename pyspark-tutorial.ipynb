{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYSPARK TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APACHE SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is an open source software developed by UC Berkley in 2009 and it was made available to the public in 2010 and since then has been used within the industry with an unprecedented scale.\n",
    "Earlier tools like MapReduce were used by Data Scientist to process streaming of data, however they were slow. To overcome this issue, spark offers a solution that is both fast and general purpose. The main difference between Spark and MapReduce is that Spark runs computations in memory while the later on the hard disk and allows high-speed access and data processing, reducing times from hours to minutes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYSPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is the name of the engine to realise cluster computing while PySpark is the Pythonâ€™s library to use Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORKING MECHANISM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark takes care of scheduling , distributing and monitoring application as it is based on computational engine. \n",
    "Each task is done across various worker machines called computing cluster. \n",
    "A computing cluster refers to the division of tasks.\n",
    "One machine performs one task, while the others contribute to the final output through a different task. \n",
    "In the end, all the tasks are aggregated to produce an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADVANTAGES OF SPARK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pyspark handles the complexities of multiprocessing, such as distributing the data, distributing code and collecting output     from the workers on a cluster of machines.\n",
    "\n",
    "- It can build an architecture that encompasses data streaming management, seamlessly data queries, machine learning               prediction and real-time access to various analysis.\n",
    "\n",
    "- Spark works closely with SQL language, i.e., structured data. It allows querying the data in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALLATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We assume that you have already installed anaconda and have some basic knowledge of python and sql to follow along with the tutorial.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to <a href=\"http://spark.apache.org/downloads.html\">Apache Spark</a> website.\n",
    "2. Choose a spark release. In our case we have used 2.4.3 (May 07 2019).\n",
    "3. Choose a package type. It will be selected by default.\n",
    "4. Click the Download spark link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://ibb.co/XF4khrx\"><img src=\"https://i.ibb.co/nQ08HNM/spark1.png\" alt=\"spark1\" border=\"0\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow the link to install <a href=\"https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c\">Spark.</a>\n",
    "\n",
    "Credits: Michael Galarnyk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: Remember to change the spark-2.1.0-bin-hadoop2.7 to spark-2.4.3-bin-hadoop2.7 where it is applicable as we are using the latest version of spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Context is the internal engine that that initiates connection with clusters. It sets up internal services and establishes connection with spark execution environment. Through SparkContext you can create RDDs, broadcast variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-OEAIEUG0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2289cf9fe48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark  #Gives the name of the Spark version running, Master and AppName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the SparkContext is initialised, you can create a RDD. \n",
    "RDDs are said to be the building blocks for Spark. \n",
    "The fullform is:\n",
    "R - Ressilient: It is fault tolerant and can rebuild data on failure\n",
    "D - Distributed: Data is distributed among multiple nodes on a cluster\n",
    "D - Dataset: Collection of partioned data with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nums= sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nums is a RDD and the code uses parallelize to tranfer chunks of data to various nodes. \n",
    "By default the total data is divided into 128MB per node. \n",
    "Example: If we have 1GB of data then it is divided into (1000/128) approximately 8 nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMATION AND ACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or more RDD as output. Each time it creates new RDD when we apply any transformation. Thus, the so input RDDs, cannot be changed since RDD are immutable in nature.\n",
    "\n",
    "Transformations are lazy in nature i.e., they get execute when we call an action. They are not executed immediately.\n",
    "\n",
    "Common Transformations are map, flatMap, filter, distinct, reduceByKey, sortBy\n",
    "\n",
    "Common Actions are collect, reduce, take, countByKey/countByValue, show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add a tranformation using a lambda function. In the example below, you are returning the square of nums.\n",
    "Here map is the tranformation and collect is the action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "squared = nums.map(lambda x: x*x).collect()\n",
    "for num in squared:\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLContext allows connecting the engine with different data sources. \n",
    "It is used to initiate the functionalities of Spark SQL.\n",
    "Once SqlContext is intialised, you can create a Data Frame in Pyspark.\n",
    "Data Frames are Built on top of RDDs and are different from panda data frames as they are immutable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using textFile method and passing the text file to read as an argument we can get the contents of the file into rdd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**type** function is used to check the datatype of the variable that is passed as an argument to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: The output will be a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"alice_in_wonderland.txt\")\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading csv if InferSchema is not set to true, then Spark does not guess the data type and by default shows String for all columns of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: The output will be a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1= sqlContext.read.csv(\"adult.csv\", header=True)\n",
    "type(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**printSchema()** method is used to display  the schema in a tree format where we can know the datatypes of all the columns and whether they can accept null values or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: string (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: string (nullable = true)\n",
      " |-- capital-loss: string (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **solve** this, we you put **inferSchema= True**, then Spark automatically guesses the type of data.\n",
    "By **default**, it is taken as **false**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(\"adult.csv\", header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- native-country: integer (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHOW DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show()** method is used to show the contents of the dataframe. By **default** it will display only **20 rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+-------------+-------------+--------------------+------------------+--------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "|age|        workclass|fnlwgt|    education|education-num|      marital-status|        occupation|  relationship|               race|    sex|capital-gain|capital-loss|native-country|         class| label|\n",
      "+---+-----------------+------+-------------+-------------+--------------------+------------------+--------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "| 39|        State-gov| 77516|    Bachelors|           13|       Never-married|      Adm-clerical| Not-in-family|              White|   Male|        2174|           0|            40| United-States| <=50K|\n",
      "| 50| Self-emp-not-inc| 83311|    Bachelors|           13|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male|           0|           0|            13| United-States| <=50K|\n",
      "| 38|          Private|215646|      HS-grad|            9|            Divorced| Handlers-cleaners| Not-in-family|              White|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 53|          Private|234721|         11th|            7|  Married-civ-spouse| Handlers-cleaners|       Husband|              Black|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 28|          Private|338409|    Bachelors|           13|  Married-civ-spouse|    Prof-specialty|          Wife|              Black| Female|           0|           0|            40|          Cuba| <=50K|\n",
      "| 37|          Private|284582|      Masters|           14|  Married-civ-spouse|   Exec-managerial|          Wife|              White| Female|           0|           0|            40| United-States| <=50K|\n",
      "| 49|          Private|160187|          9th|            5| Married-spouse-a...|     Other-service| Not-in-family|              Black| Female|           0|           0|            16|       Jamaica| <=50K|\n",
      "| 52| Self-emp-not-inc|209642|      HS-grad|            9|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male|           0|           0|            45| United-States|  >50K|\n",
      "| 31|          Private| 45781|      Masters|           14|       Never-married|    Prof-specialty| Not-in-family|              White| Female|       14084|           0|            50| United-States|  >50K|\n",
      "| 42|          Private|159449|    Bachelors|           13|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male|        5178|           0|            40| United-States|  >50K|\n",
      "| 37|          Private|280464| Some-college|           10|  Married-civ-spouse|   Exec-managerial|       Husband|              Black|   Male|           0|           0|            80| United-States|  >50K|\n",
      "| 30|        State-gov|141297|    Bachelors|           13|  Married-civ-spouse|    Prof-specialty|       Husband| Asian-Pac-Islander|   Male|           0|           0|            40|         India|  >50K|\n",
      "| 23|          Private|122272|    Bachelors|           13|       Never-married|      Adm-clerical|     Own-child|              White| Female|           0|           0|            30| United-States| <=50K|\n",
      "| 32|          Private|205019|   Assoc-acdm|           12|       Never-married|             Sales| Not-in-family|              Black|   Male|           0|           0|            50| United-States| <=50K|\n",
      "| 40|          Private|121772|    Assoc-voc|           11|  Married-civ-spouse|      Craft-repair|       Husband| Asian-Pac-Islander|   Male|           0|           0|            40|             ?|  >50K|\n",
      "| 34|          Private|245487|      7th-8th|            4|  Married-civ-spouse|  Transport-moving|       Husband| Amer-Indian-Eskimo|   Male|           0|           0|            45|        Mexico| <=50K|\n",
      "| 25| Self-emp-not-inc|176756|      HS-grad|            9|       Never-married|   Farming-fishing|     Own-child|              White|   Male|           0|           0|            35| United-States| <=50K|\n",
      "| 32|          Private|186824|      HS-grad|            9|       Never-married| Machine-op-inspct|     Unmarried|              White|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 38|          Private| 28887|         11th|            7|  Married-civ-spouse|             Sales|       Husband|              White|   Male|           0|           0|            50| United-States| <=50K|\n",
      "| 43| Self-emp-not-inc|292175|      Masters|           14|            Divorced|   Exec-managerial|     Unmarried|              White| Female|           0|           0|            45| United-States|  >50K|\n",
      "+---+-----------------+------+-------------+-------------+--------------------+------------------+--------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show particular number of rows we can pass that **number** as an argument to **show()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|age|        workclass|fnlwgt| education|education-num|     marital-status|        occupation|  relationship|  race|    sex|capital-gain|capital-loss|native-country|         class| label|\n",
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "| 39|        State-gov| 77516| Bachelors|           13|      Never-married|      Adm-clerical| Not-in-family| White|   Male|        2174|           0|            40| United-States| <=50K|\n",
      "| 50| Self-emp-not-inc| 83311| Bachelors|           13| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|           0|           0|            13| United-States| <=50K|\n",
      "| 38|          Private|215646|   HS-grad|            9|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 53|          Private|234721|      11th|            7| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 28|          Private|338409| Bachelors|           13| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|           0|           0|            40|          Cuba| <=50K|\n",
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECT COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**select()** method is used to select only the specified columns which are passed as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see **select** working as a **transformation** which transform df to a **smaller version** of **df** and we apply **action** to the **smaller version of df** which is **show**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|fnlwgt|\n",
      "+---+------+\n",
      "| 39| 77516|\n",
      "| 50| 83311|\n",
      "| 38|215646|\n",
      "| 53|234721|\n",
      "| 28|338409|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','fnlwgt').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUPING VALUES AND APPLYING COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupBy()** method allows you to group **rows** of **data together** and then apply **functions** such as **count()** or **sum()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    education|count|\n",
      "+-------------+-----+\n",
      "|  Prof-school|  576|\n",
      "|         10th|  933|\n",
      "|      7th-8th|  646|\n",
      "|      5th-6th|  333|\n",
      "|   Assoc-acdm| 1067|\n",
      "|    Assoc-voc| 1382|\n",
      "|      Masters| 1723|\n",
      "|         12th|  433|\n",
      "|    Preschool|   51|\n",
      "|          9th|  514|\n",
      "|    Bachelors| 5355|\n",
      "|    Doctorate|  413|\n",
      "|      HS-grad|10501|\n",
      "|         11th| 1175|\n",
      "| Some-college| 7291|\n",
      "|      1st-4th|  168|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the values of count are not easy to read. To **solve** this we can simply sort the values in either ascending or descending order using **sort()** method and passing the column to be sorted as the first argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default **sort()** method will **sort** the column in **ascending** order. To **change** it to **descending** order we simply set **ascending** = **False** in the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    education|count|\n",
      "+-------------+-----+\n",
      "|      HS-grad|10501|\n",
      "| Some-college| 7291|\n",
      "|    Bachelors| 5355|\n",
      "|      Masters| 1723|\n",
      "|    Assoc-voc| 1382|\n",
      "|         11th| 1175|\n",
      "|   Assoc-acdm| 1067|\n",
      "|         10th|  933|\n",
      "|      7th-8th|  646|\n",
      "|  Prof-school|  576|\n",
      "|          9th|  514|\n",
      "|         12th|  433|\n",
      "|    Doctorate|  413|\n",
      "|      5th-6th|  333|\n",
      "|      1st-4th|  168|\n",
      "|    Preschool|   51|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().sort(\"count\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code, will **group** the **data** based on **martial status** and then we applied **aggregate function** named **mean** on the column **captial gain** using **agg()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|      marital-status| avg(capital-gain)|\n",
      "+--------------------+------------------+\n",
      "|             Widowed| 571.0715005035247|\n",
      "| Married-spouse-a...| 653.9832535885167|\n",
      "|   Married-AF-spouse| 432.6521739130435|\n",
      "|  Married-civ-spouse|1764.8595085470085|\n",
      "|            Divorced| 728.4148098131893|\n",
      "|       Never-married|376.58831788823363|\n",
      "|           Separated| 535.5687804878049|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('marital-status').agg({'capital-gain': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIBE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**describe()** method gives **summary** **statistics** of the **data**:\n",
    "1. **count**,\n",
    "2. **mean**,\n",
    "3. **standard deviation**,\n",
    "4. **min**, \n",
    "5. **max**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+-------------+-----------------+--------------+-----------------+------------+-------------------+-------+------------------+----------------+------------------+-----------+\n",
      "|summary|               age|   workclass|            fnlwgt|    education|    education-num|marital-status|       occupation|relationship|               race|    sex|      capital-gain|    capital-loss|    native-country|      class|\n",
      "+-------+------------------+------------+------------------+-------------+-----------------+--------------+-----------------+------------+-------------------+-------+------------------+----------------+------------------+-----------+\n",
      "|  count|             32561|       32561|             32561|        32561|            32561|         32561|            32561|       32561|              32561|  32561|             32561|           32561|             32561|      32561|\n",
      "|   mean| 38.58164675532078|        null|189778.36651208502|         null| 10.0806793403151|          null|             null|        null|               null|   null|1077.6488437087312| 87.303829734959|40.437455852092995|       null|\n",
      "| stddev|13.640432553581356|        null|105549.97769702227|         null|2.572720332067397|          null|             null|        null|               null|   null| 7385.292084840354|402.960218649002|12.347428681731838|       null|\n",
      "|    min|                17|           ?|           12285.0|         10th|              1.0|      Divorced|                ?|     Husband| Amer-Indian-Eskimo| Female|               0.0|             0.0|               1.0|          ?|\n",
      "|    max|                90| Without-pay|         1484705.0| Some-college|             16.0|       Widowed| Transport-moving|        Wife|              White|   Male|           99999.0|          4356.0|              99.0| Yugoslavia|\n",
      "+-------+------------------+------------+------------------+-------------+-----------------+--------------+-----------------+------------+-------------------+-------+------------------+----------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **view** the **statistics** of a **particular** column we pass that **column name** as **argument** into the **describe()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      capital-gain|\n",
      "+-------+------------------+\n",
      "|  count|             32561|\n",
      "|   mean|1077.6488437087312|\n",
      "| stddev| 7385.292084840354|\n",
      "|    min|                 0|\n",
      "|    max|             99999|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('capital-gain').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter()** method is used to **filter** the **orginial dataframe** on the basis of an **condition**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we are to find the number of people who are aged greater than 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+----------+-------------+--------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|age|        workclass|fnlwgt| education|education-num|      marital-status|        occupation|  relationship|  race|    sex|capital-gain|capital-loss|native-country|         class| label|\n",
      "+---+-----------------+------+----------+-------------+--------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "| 50| Self-emp-not-inc| 83311| Bachelors|           13|  Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|           0|           0|            13| United-States| <=50K|\n",
      "| 53|          Private|234721|      11th|            7|  Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 49|          Private|160187|       9th|            5| Married-spouse-a...|     Other-service| Not-in-family| Black| Female|           0|           0|            16|       Jamaica| <=50K|\n",
      "| 52| Self-emp-not-inc|209642|   HS-grad|            9|  Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|           0|           0|            45| United-States|  >50K|\n",
      "| 42|          Private|159449| Bachelors|           13|  Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|        5178|           0|            40| United-States|  >50K|\n",
      "+---+-----------------+------+----------+-------------+--------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age > 40).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13443"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.age > 40).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Functionality of SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the famous **Flight Delays** dataset and try to answer the **two** main **questions**.\n",
    "\n",
    "- Print the **Arrival Delay** data for the **flights** that **departs** from **San Fransico (SFO)**.\n",
    "- List the **top 3 destination airports** (by no. of arriving planes) showing the **count** of **arriving planes** with **Destination code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nycflight_df = spark.read.csv(\"flightdelays.csv\",inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008|    1|         3|        4|   2003|      1955|   2211|      2225|           WN|      335| N712SW|              128|           150|    116|     -14|       8|   IAD| TPA|     810|     4|      8|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|    754|       735|   1002|      1000|           WN|     3231| N772SW|              128|           145|    113|       2|      19|   IAD| TPA|     810|     5|     10|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|    628|       620|    804|       750|           WN|      448| N428WN|               96|            90|     76|      14|       8|   IND| BWI|     515|     3|     17|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|    926|       930|   1054|      1100|           WN|     1746| N612SW|               88|            90|     78|      -6|      -4|   IND| BWI|     515|     3|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|   1829|      1755|   1959|      1925|           WN|     3920| N464WN|               90|            90|     77|      34|      34|   IND| BWI|     515|     3|     10|        0|            null|       0|           2|           0|       0|            0|               32|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nycflight_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code will show how many **observation** we have in our **dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7009728"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nycflight_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The sql function on a SQLContext enables applications to run SQL queries programmatically and returns the result as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To run sql queries on our dataset we first to create a temporary table which will accessed by the sqlContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the above by using **registerTempTable()** method and **pass** the **name of the table** that we will **refer** to when **executing** different **queries**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nycflight_df.registerTempTable('flighttable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008|    1|         3|        4|   2003|      1955|   2211|      2225|           WN|      335| N712SW|              128|           150|    116|     -14|       8|   IAD| TPA|     810|     4|      8|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|    754|       735|   1002|      1000|           WN|     3231| N772SW|              128|           145|    113|       2|      19|   IAD| TPA|     810|     5|     10|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|    628|       620|    804|       750|           WN|      448| N428WN|               96|            90|     76|      14|       8|   IND| BWI|     515|     3|     17|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|    926|       930|   1054|      1100|           WN|     1746| N612SW|               88|            90|     78|      -6|      -4|   IND| BWI|     515|     3|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|   1829|      1755|   1959|      1925|           WN|     3920| N464WN|               90|            90|     77|      34|      34|   IND| BWI|     515|     3|     10|        0|            null|       0|           2|           0|       0|            0|               32|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select * from flighttable').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below **query** will give the **result** to the **first** question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|origin|arrdelay|\n",
      "+------+--------+\n",
      "|   SFO|      69|\n",
      "|   SFO|      46|\n",
      "|   SFO|     120|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select origin,arrdelay from flighttable where origin = 'SFO' limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below **query** will give the **result** to the **second** question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Dest|count(Dest)|\n",
      "+----+-----------+\n",
      "| ATL|     407816|\n",
      "| ORD|     334358|\n",
      "| DFW|     273685|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select Dest, Count(Dest) from flighttable where Cancelled = 0 Group By DEST order by Count(Dest) desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We noticed that the **run time** to get the **same output** using **python script** on **command line** shell was **more** than the **time taken** to **execute** the **above code**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY SPARK??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a 1gb of text file named dummy.txt using the following command into the cmd:\n",
    "\n",
    "- echo \"This is just a sample line appended to create a big file.. \" > dummy.txt for /L %i in (1,1,24) do type dummy.txt >> dummy.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the file we will read it using what we learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contentRDD =sc.textFile(\"dummy.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code, will **filter** the rows which has **length 0**. So all the **lines** having no **character** in it will be **filtered** out and the **remaining lines** will be **selected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nonempty_lines = contentRDD.filter(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use **split()** based on **space** character to get individual words of the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = nonempty_lines.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the individual words, so we can now count the occurence of words.\n",
    "- Apply the **map** method to create the following (individual word, 1) for all the words.\n",
    "- Apply the **reduceByKey** method to add the values of individual word kind of grouping the words and use sum afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*%%time* is used to calcualte the time taken to run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note: The time taken to do the count is only 199ms for a 1 GB of text file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wordcount = words.map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **save** the output of the above code to a text file we simply use **saveAsTextFile()**. **coalesce()** is used to tell spark to only create a single output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why spark creates multiple output if **coalesce()** is not used is because the code is executed on blocks resulting into multiple outputs. Hence, we specify the **coalesce()** method and passing **1** stating that create only 1 output file and **shuffle=True** will store the output in a random order in the outputted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcount.coalesce(1,shuffle=True).saveAsTextFile(\"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is another way to get the output. The output will be displayed here only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 16777216)\n",
      "('create', 16777216)\n",
      "('big', 16777216)\n",
      "('file..', 16777216)\n",
      "('a', 33554432)\n",
      "('just', 16777216)\n",
      "('to', 16777216)\n",
      "('is', 16777216)\n",
      "('\"', 16777216)\n",
      "('sample', 16777216)\n",
      "('appended', 16777216)\n",
      "('\"This', 16777216)\n",
      "('line', 16777216)\n"
     ]
    }
   ],
   "source": [
    "for word in wordcount.collect():\n",
    "\n",
    "     print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets count the occurence of word without using spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open(\"dummy.txt\")\n",
    "wordcount1 = {}\n",
    "\n",
    "for word in file.read().split():\n",
    "    if word not in wordcount1:\n",
    "        wordcount1[word] = 1\n",
    "    else:\n",
    "        wordcount1[word] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we can see the time taken to output the same result is 1 minute and 16s which is way more than 199ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"This': 16777216,\n",
       " 'is': 16777216,\n",
       " 'just': 16777216,\n",
       " 'a': 33554432,\n",
       " 'sample': 16777216,\n",
       " 'line': 16777216,\n",
       " 'appended': 16777216,\n",
       " 'to': 16777216,\n",
       " 'create': 16777216,\n",
       " 'big': 16777216,\n",
       " 'file..': 16777216,\n",
       " '\"': 16777216}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn different terms such as **SparkContext**, **RDDs**, **Transformation/Actions** and using methods like **show()**, **groupBy()**, etc. we have used the **<a href=\"https://www.guru99.com/pyspark-tutorial.html#5\">guru99</a>** website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the **word count** example we have used the **<a href=\"http://geoinsyssoft.com/pyspark-wordcount-program/\">geoinsyssoft's</a>** website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
